mGoog <- to.monthly(GOOG[,1])
googOpen <- Op(mGoog)
ts1 <- ts(googOpen,frequency=12)
plot(ts1,xlab="Years+1", ylab="GOOG")
plot(decompose(ts1),xlab="Years+1")
ts1Train <- window(ts1,start=1,end=5)
ts1Test <- window(ts1,start=5,end=(7-0.01))
ts1Train
ts1Train <- window(ts1,start=1,end=5)
plot(ts1Train)
lines(ma(ts1Train,order=3),col="red")
install.packages("forecast")
library(forecast)
plot(ts1Train)
lines(ma(ts1Train,order=3),col="red")
library(quantmod)
from.dat <- as.Date("01/01/08", format="%m/%d/%y")
to.dat <- as.Date("12/31/13", format="%m/%d/%y")
getSymbols("GOOG", src="google", from = from.dat, to = to.dat)
head(GOOG)
## Summarize monthly and store as time series
mGoog <- to.monthly(GOOG[,1])
googOpen <- Op(mGoog)
ts1 <- ts(googOpen,frequency=12)
plot(ts1,xlab="Years+1", ylab="GOOG")
plot(decompose(ts1),xlab="Years+1")
ts1Train <- window(ts1,start=1,end=5)
ts1Test <- window(ts1,start=5,end=(7-0.01))
ts1Train
library(forecast)
plot(ts1Train)
lines(ma(ts1Train,order=3),col="red")
ets1 <- ets(ts1Train,model="MMM")
fcast <- forecast(ets1)
plot(fcast); lines(ts1Test,col="red")
accuracy(fcast,ts1Test)
data(iris); library(ggplot2); library(caret)
inTrain <- createDataPartition(y=iris$Species,
p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training); dim(testing)
kMeans1 <- kmeans(subset(training,select=-c(Species)),centers=3)
training$clusters <- as.factor(kMeans1$cluster)
qplot(Petal.Width,Petal.Length,colour=clusters,data=training)
table(kMeans1$cluster,training$Species)
modFit <- train(clusters ~.,data=subset(training,select=-c(Species)),method="rpart")
table(predict(modFit,training),training$Species)
testClusterPred <- predict(modFit,testing)
table(testClusterPred ,testing$Species)
library(caret)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
training <- vowel.train
testing <- vowel.test
training$y <- as.factor(training$y)
testing$y <- as.factor(testing$y)
set.seed(33833)
modFit <- train(y~ .,data=training,method="rf",prox=TRUE)
modFit2 <â€ train(y~ ., method="gbm",data=training,verbose=FALSE)
predict(modFit, testing)
predict(modFit2, testing)
pred1 <- predict(modFit,testing); pred2 <- predict(modFit2,testing)
qplot(pred1,pred2,colour=wage,data=testing)
qplot(pred1,pred2,colour=y,data=testing)
predDF <- data.frame(pred1,pred2,wage=testing$wage)
predDF <- data.frame(pred1,pred2,wage=testing$y)
combModFit <- train(wage ~.,method="gam",data=predDF)
combPred <- predict(combModFit,predDF)
sqrt(sum((pred1-testing$y)^2))
pred1 == testing
pred1 == testing$y
sum(pred1 == testing$y) / length(testing$y)
sum(pred2 == testing$y) / length(testing$y)
sum(combPred == testing$y) / length(testing$y)
predDF <- data.frame(pred1,pred2,wage=testing$y)
combModFit <- train(y~.,method="gam",data=predDF)
combModFit <- train(y~.,method="gam",data=predDF)
predDF <- data.frame(pred1,pred2,y=testing$y)
combModFit <- train(y~.,method="gam",data=predDF)
combPred <- predict(combModFit,predDF)
sum(pred1 == testing$y) / length(testing$y)
sum(pred2 == testing$y) / length(testing$y)
sum(combPred == testing$y) / length(testing$y)
agree <- pred1 == pred2
agree
agreePreDF <- predDF[agree,]
sum(agreePreDF$pred1 == agreePreDF$y) / length(agreePreDF$y)
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
head(training)
set.seed(62433)
modgbm <- train(diagnosis~.,method="gbm",data=training, verbose = FALSE)
modrf <- train(diagnosis~.,method="rf",
data=training,
trControl = trainControl(method="cv"),number=3) #takes too long to run
modlda <- train(diagnosis~.,method="lda",data=training)
modlda <- train(diagnosis~.,method="lda",data=training)
predgbm <- predict(modgbm,testing); predrf <- predict(modrf,testing);
predlda <- predict(modlda, testing)
qplot(predgbm,predrf,colour=diagnosis,data=testing)
predDF <- data.frame(predgbm,predrf, predlda,diagnosis=testing$diagnosis)
combModFit <- train(diagnosis~.,method="gam",data=predDF)
combPred <- predict(combModFit,predDF)
predDF$combo <- combPred
sum(predDF$combo == predDF$diagnosis) / length(predDF$diagnosis)
sum(predDF$predgbm == predDF$diagnosis) / length(predDF$diagnosis)
sum(predDF$predrf == predDF$diagnosis) / length(predDF$diagnosis)
sum(predDF$predlda == predDF$diagnosis) / length(predDF$diagnosis)
url <- https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv
setwd("~/Data_Science_Specialization/8_MachineLearning")
setwd("~/Data_Science_Specialization/8_MachineLearning")
if (!file.exists("data")) {
dir.create("data")
}
fileUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv"
download.file(fileUrl, destfile = "./data/gaData.csv")
list.files("./data")
library(lubridate) # For year() function below
dat = read.csv("~/Desktop/gaData.csv")
dat = read.csv("./data/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
plot(decompose(ts1),xlab="Years+1")
library(quantmod)
from.dat <- as.Date("01/01/08", format="%m/%d/%y")
to.dat <- as.Date("12/31/13", format="%m/%d/%y")
getSymbols("GOOG", src="google", from = from.dat, to = to.dat)
head(GOOG)
mGoog <- to.monthly(GOOG[,1])
googOpen <- Op(mGoog)
ts1 <- ts(googOpen,frequency=12) #creates a time series
plot(ts1,xlab="Years+1", ylab="GOOG")
plot(decompose(ts1),xlab="Years+1")
ts1Train <- window(ts1,start=1,end=5)
ts1Test <- window(ts1,start=5,end=(7-0.01))
ts1Train
library(forecast)
plot(ts1Train)
lines(ma(ts1Train,order=3),col="red") #ma is moving average
ets1 <- ets(ts1Train,model="MMM")
fcast <- forecast(ets1)
plot(fcast); lines(ts1Test,col="red")
accuracy(fcast,ts1Test)
?bats
data("USAccDeaths")
data <- USAccDeaths
head(data)
fit <- bats(USAccDeaths, use.parallel = FALSE)
fit
USAccDeaths
plot(tstrain)
library(lubridate) # For year() function below
dat = read.csv("./data/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr) #makes it a time series
# Fit a model using the bats() function in the forecast package to the training
# time series. Then forecast this model for the remaining time points. For how
# many of the testing points is the true value within the 95% prediction interval
# bounds?
library(forecast)
plot(tstrain)
lines(ma(tstrain,order=3),col="red") #ma is moving average
?ets
bats1 <- bats(tstrain)
fcast <- forecast(bats1)
tstrain = ts(testing$visitsTumblr) #makes it a time series
plot(fcast); lines(tstrain, col = "red")
plot(fcast); lines(tstest, col = "red")
dat = read.csv("./data/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr) #makes it a time series
tstest = ts(testing$visitsTumblr) #makes it a time series
# Fit a model using the bats() function in the forecast package to the training
# time series. Then forecast this model for the remaining time points. For how
# many of the testing points is the true value within the 95% prediction interval
# bounds?
library(forecast)
plot(tstrain)
lines(ma(tstrain,order=3),col="red") #ma is moving average
## BATS model:
bats1 <- bats(tstrain)
fcast <- forecast(bats1)
plot(fcast); lines(tstest, col = "red")
accuracy(fcast, tstest)
tstrain
training
head(dat)
head(training)
tail(training)
head(testing)
tail(testing)
bats1 <- bats(tstrain)
fcast <- forecast(bats1)
plot(fcast); lines(tstest, col = "red")
accuracy(fcast, tstest)
accuracy(fcast, tstrain)
bats1 <- bats(tstrain)
fcast <- forecast(bats1)
plot(fcast); lines(tstrain, col = "red")
accuracy(fcast, tstrain)
accuracy(fcast, tstest)
library(quantmod)
from.dat <- as.Date("01/01/08", format="%m/%d/%y")
to.dat <- as.Date("12/31/13", format="%m/%d/%y")
getSymbols("GOOG", src="google", from = from.dat, to = to.dat)
head(GOOG)
mGoog <- to.monthly(GOOG[,1])
googOpen <- Op(mGoog)
?Op
head(googOpen)
ts1 <- ts(googOpen,frequency=12) #creates a time series
head(ts1)
ts1
plot(ts1,xlab="Years+1", ylab="GOOG")
plot(decompose(ts1),xlab="Years+1")
ts1Train <- window(ts1,start=1,end=5)
ts1Test <- window(ts1,start=5,end=(7-0.01))
ts1Train
ts1Test
ets1 <- ets(ts1Train,model="MMM")
fcast <- forecast(ets1)
plot(fcast); lines(ts1Test,col="red")
accuracy(fcast,ts1Test)
visits.exp.smoothing = bats(tstrain)
visits.forecast = forecast(visits.exp.smoothing, nrow(testing))
# plot the forecast
plot(visits.forecast)
visits.forecast.lower95 = visits.forecast$lower[,2]
visits.forecast.upper95 = visits.forecast$upper[,2]
table (
(testing$visitsTumblr>visits.forecast.lower95) &
(testing$visitsTumblr<visits.forecast.upper95))
226/nrow(testing)
nrow(testing)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
library(caret)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
library("e1071")
set.seed(325)
conc.fit.svm = svm(CompressiveStrength ~ .,
data=training)
# comparing predictions to actual values
conc.pred.svm = predict(conc.fit.svm, newdata = testing)
error = conc.pred.svm - testing$CompressiveStrength
sqrt(mean(error^2))
plot(conc.pred.svm, testing$CompressiveStrength,
pch=20, cex=1,
col=testing$Age,
main="Relationship between the svm forecast and actual values")
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
# Set the seed to 233 and fit a lasso model to predict Compressive Strength.
# Which variable is the last coefficient to be set to zero as the penalty
# increases? (Hint: it may be useful to look up ?plot.enet).
set.seed(233)
conc.fit.lasso = train(CompressiveStrength ~ .,
data = training,
method = "lasso")
plot.enet(conc.fit.lasso$finalModel,
xvar="penalty", use.color=TRUE)
conc.fit.lasso$finalModel$beta.pure
setwd("~/Data_Science_Specialization/8_MachineLearning/FinalProject/")
if (!file.exists("data")) {
dir.create("data")
}
setwd("~/Data_Science_Specialization/8_MachineLearning/FinalProject/")
if (!file.exists("data")) {
dir.create("data")
}
urlTrain <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
urlTest <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(urlTrain, destfile = "./data/training.csv")
download.file(urlTest, destfile = "./data/testing.csv")
list.files("./data")
training <- read.csv("./data/training.csv")
testing <- read.csv("./data/testing.csv")
head(training)
colnames(training)
head(training)[1:5, 1:6]
head(training)[1:5, 1:10]
head(training)[1:5, 1:20]
colnames(training)
library(kernlab)
data(spam)
head(spam)
plot(density(spam$your[spam$type=="nonspam"]),
col="blue",main="",xlab="Frequency of 'your'")
lines(density(spam$your[spam$type=="spam"]),col="red")
#set cutoff for SPAM
plot(density(spam$your[spam$type=="nonspam"]),
col="blue",main="",xlab="Frequency of 'your'")
lines(density(spam$your[spam$type=="spam"]),col="red")
abline(v=0.5,col="black")
set.seed(32343)
modelFit <- train(type ~.,data=training, method="glm")
library(caret); library(kernlab); data(spam)
#says use 75% for training and 25% for testing
inTrain <- createDataPartition(y=spam$type,
p=0.75, list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
dim(training)
set.seed(32343)
modelFit <- train(type ~.,data=training, method="glm")
modelFit$finalModel
predictions <- predict(modelFit,newdata=testing)
predictions
confusionMatrix(predictions,testing$type)
library(caret)
library(caret);data(faithful); set.seed(333)
inTrain <- createDataPartition(y=faithful$waiting,
p=0.5, list=FALSE)
trainFaith <- faithful[inTrain,]; testFaith <- faithful[-inTrain,]
head(trainFaith) #we have eruption time an waiting time
modFit <- train(eruptions ~ waiting,data=trainFaith,method="lm")
summary(modFit$finalModel)
training <- read.csv("./data/training.csv")
testing <- read.csv("./data/testing.csv")
View(testing)
View(training)
colnames(training)
View(training[,84:160])
columns <- c(1,8:11, 37:49,60:68,84:86, 102, 113:124,140,151:160)
columns
training <- training[,columns]
testing <- testing[,columns]
colnames(training)
testing
complete.cases(training)
sum(complete.cases(training))
length(training$X)
sum(complete.cases(training)) - length(training$X)
training <- read.csv("./data/training.csv")
testing <- read.csv("./data/testing.csv")
columns <- c(8:11, 37:49,60:68,84:86, 102, 113:124,140,151:160)
training <- training[,columns]
testing <- testing[,columns]
colnames(training) #note column 53 is the classe.
sum(complete.cases(training)) - length(training$X) #Sum is zero therefore there are no
sum(complete.cases(training)) - length(training$roll_belt) #Sum is zero therefore there are no
set.seed(32343)
modelFit <- train(classe ~.,data=training, method="glm")
training <- read.csv("./data/training.csv")
testing <- read.csv("./data/testing.csv")
columns <- c(8:11, 37:49,60:68,84:86, 102, 113:124,140,151:160)
training <- training[,columns]
testing <- testing[,columns]
colnames(training) #note column 53 is the classe.
sum(complete.cases(training)) - length(training$roll_belt) #Sum is zero therefore there are no
str(training)
library(caret); library(kernlab); data(spam)
#says use 75% for training and 25% for testing
inTrain <- createDataPartition(y=spam$type,
p=0.75, list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
dim(training)
set.seed(32343)
modelFit <- train(type ~.,data=training, method="glm")
modelFit
modelFit$finalModel
predictions <- predict(modelFit,newdata=testing)
predictions
confusionMatrix(predictions,testing$type)
typeColor <- ((spam$type=="spam")*1 + 1)
prComp <- prcomp(log10(spam[,-58]+1)) #princ components on entire dataset
plot(prComp$x[,1],prComp$x[,2],col=typeColor,xlab="PC1",ylab="PC2")
training <- read.csv("./data/training.csv")
testing <- read.csv("./data/testing.csv")
columns <- c(8:11, 37:49,60:68,84:86, 102, 113:124,140,151:160)
training <- training[,columns]
testing <- testing[,columns]
colnames(training) #note column 53 is the classe.
sum(complete.cases(training)) - length(training$roll_belt) #Sum is zero therefore there are no
modelFit <- train(classe ~.,data=training,
preProcess=c("center","scale"),method="glm") #standardize
modelFit <- train(training$classe ~ .,method="glm",preProcess="pca",data=training)
training$classe
summary(training$classe)
str(training)
modFit <- train(classe ~.,data=subset(training,select=-c(1:20)),method="rpart")
modFit
predictions <- predict(modFit,newdata=testing)
confusionMatrix(predictions,testing$classe)
predictions
confusionMatrix(predictions,testing$classe)
testing$classe
testing
testing <- read.csv("./data/testing.csv")
testing
colnames(testing)
testing$class
testing$classe
View(testing)
View(testing[,100:160])
urlTest <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(urlTest, destfile = "./data/testing.csv")
list.files("./data")
testing <- read.csv("./data/testing.csv")
View(testing)
View(testing[80:160])
urlValid <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(urlTrain, destfile = "./data/training.csv")
data <- read.csv("./data/training.csv")
data <- read.csv("./data/training.csv")
valid <- read.csv("./data/Valid.csv")
download.file(urlTest, destfile = "./data/Valid.csv")
list.files("./data")
valid <- read.csv("./data/Valid.csv")
inTrain <- createDataPartition(y=data$classe,
p=0.7, list=FALSE)
training <- data[inTrain,]
testing <- data[-inTrain,]
dim(training); dim(testing)
columns <- c(8:11, 37:49,60:68,84:86, 102, 113:124,140,151:160)
training <- training[,columns]
testing <- testing[,columns]
colnames(training) #note column 53 is the classe.
sum(complete.cases(training)) - length(training$roll_belt) #Sum is zero therefore there are no
modFit = train(classe ~ ., data=training,method="nb") #nb is naive bayes
dim(training); dim(testing)
View(training[1:10,])
View(data[1:5,])
set.seed(1234)
fitCon <-trainControl(method="cv", number=5, allowParallel=T, verbose=T)
rffit<-train(classe~.,data=training, method="rf", trControl=fitCon, verbose=F)
modFit1 <- rffit
modFit <- rffit
predictions <- predict(modFit,newdata=testing)
predictions
confusionMatrix(predictions,testing$classe)
cv <- trainControl(method="cv", number=3)
modlda = train(classe ~ .,data=training,method="lda", trContol = cv)
predict(modlda, newdata = testing)
prediction_lda <- predict(modlda, newdata = testing)
prediction_lda
confusionMatrix(prediction_lda, testing$classe)
modFit = train(classe ~ ., data=training,method="nb", trContol = cv) #nb is naive bayes
predictions <- predict(modFit,newdata=testing)
confusionMatrix(predictions,testing$classe)
mod_nb = train(classe ~ ., data=training,method="nb", trContol = cv) #nb is naive bayes
mod_nb
modFit <- rffit
modFit2 <- train(classe~.,data=training, method="rf", trControl=cv, verbose=F)
predictions <- predict(modFit2,newdata=testing)
confusionMatrix(predictions,testing$classe)
modFit <- train(classe ~ ., method="gbm",data=training,verbose=FALSE, trConrol = cv)
modFit3 <- train(classe ~ ., method="gbm",data=training,verbose=FALSE, trConrol = cv)
modelFit <- train(classe ~ .,method="glm",preProcess="pca",data=training, trControl = cv)
modFit3 <- train(classe ~.,data=subset(training,select=-c(1:10)),method="rpart") #only accuracy of around 44%
predictions <- predict(modFit3,newdata=testing)
confusionMatrix(predictions,testing$classe)
modFit3 <- train(classe ~.,data=subset(training,select=-c(1:10)),method="rpart", trControl = cv) #only accuracy of around 44%
predictions <- predict(modFit3,newdata=testing)
confusionMatrix(predictions,testing$classe)
modFit4 = train(classe ~ ., data=subset(training, select =-c(1:10)),method="nb", trContol = cv) #nb is naive bayes
modFit4 = train(classe ~ ., data=subset(training, select =-c(1:10)),method="nb", trControl = cv) #nb is naive bayes
predictions <- predict(modFit4,newdata=testing)
warnings()
confusionMatrix(predictions,testing$classe)
confusionMatrix(prediction_lda, testing$classe) #only 70%
?rpart
?nba
?train
predictions <- predict(modFit2,newdata=testing)
confusionMatrix(predictions,testing$classe)
dim(training); dim(testing)
confusionMatrix(modFit2)[,1]
confusionMatrix(modFit2)[2]
confusionMatrix(modFit2)[5]
confusionMatrix(modFit2)[3]
confusionMatrix(modFit2)
confusionMatrix(prediction_lda, testing$classe)
confusionMatrix(prediction_lda, testing$classe)[1,]
confusionMatrix(prediction_lda, testing$classe)[1]
confusionMatrix(prediction_lda, testing$classe)[5]
confusionMatrix(prediction_lda, testing$classe)[6]
y <- confusionMatrix(prediction_lda, testing$classe)
y$accuracy
y$Accuracy
names(y)
y$table
y$overall
y$positive
y$positive
y$byClass
confusionMatrix(prediction_lda, testing$classe)$overall
confusionMatrix(predictions,testing$classe)
